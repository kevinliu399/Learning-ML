{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from https://www.youtube.com/watch?v=vBlO87ZAiiw&ab_channel=NeuralNine\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    transform = ToTensor(), # Tensors are similar to numpy arrays, but can also be used on a GPU to accelerate computing\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    transform = ToTensor(), # Tensors are similar to numpy arrays, but can also be used on a GPU to accelerate computing\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x1e44bbb18b0>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x1e44be41490>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader # DataLoader is an iterable that allows us to batch and shuffle the data\n",
    "\n",
    "# Create data loaders for the training and test sets\n",
    "loaders = {\n",
    "    'train' : DataLoader(train_data, batch_size = 100, shuffle = True, num_workers = 1),\n",
    "    'test' : DataLoader(test_data, batch_size = 100, shuffle = True, num_workers = 1),\n",
    "}\n",
    "\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN architecture\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Inherit from nn.Module\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()                                  # Call the constructor of the parent class\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)               # 1 input channel, 10 output channels, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
    "        self.conv2_drop = nn.Dropout2d()                             # Regularization layout to deactive certain nodes to prevent overfitting during training\n",
    "        self.fc1 = nn.Linear(320, 50)                                # 320 input features, 50 output features\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):                                            # define the activation functions\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))                   # 2x2 max pooling => this reduces the size of the image by half\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))  # we call the dropout layer here because we want to apply it after the activation function\n",
    "        x = x.view(-1, 320)                                          # flatten the tensor: 20 channels * 4x4 image size\n",
    "        x = F.relu(self.fc1(x))                                      # fully connected layer\n",
    "        x = F.dropout(x, training = self.training)                   # apply dropout layer\n",
    "        x = self.fc2(x)                                              \n",
    "        return F.log_softmax(x, dim = 1)                             # apply softmax to get probabilities so that we can interpret the output as probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)               # Stochastic Gradient Descent to optimize the model, we use lr = 0.001 as learning rate to update the weights\n",
    "loss_fn = nn.CrossEntropyLoss()                                      # CrossEntropyLoss is used for classification problems since it combines the softmax and the negative log likelihood loss\n",
    "\n",
    "def train(epoch):\n",
    "    model.train() \n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)            # move the data to the GPU if available\n",
    "        optimizer.zero_grad()                                        # set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()                                              # store the gradients in the model and update the weights by calling the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Every 10 batches\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(loaders['train'].dataset),\n",
    "                100. * batch_idx / len(loaders['train']), loss.item()))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():                                            # disable gradient calculation to speed up the computation as we don't need gradients for evaluation\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()              # sum up batch loss\n",
    "            pred = output.argmax(dim = 1, keepdim = True)            # get the index of the max log-probability. Argmax is used to get the index of the max value in a tensor\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()    # compare the prediction with the target and sum up the correct predictions\n",
    "    \n",
    "    test_loss /= len(loaders['test'].dataset)                        # calculate the average loss\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(loaders['test'].dataset),\n",
    "        100. * correct / len(loaders['test'].dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"Current CUDA device:\", torch.cuda.current_device())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304594\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 2.235417\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.139275\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 2.005163\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 1.569492\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 1.416600\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 1.145070\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 1.162713\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.829797\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 0.859266\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.797640\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 0.717850\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.574752\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 0.627447\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.635988\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.498823\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.610395\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 0.665216\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.584736\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 0.685381\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.580455\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 0.483533\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.650666\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 0.529277\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.455221\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.465333\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.406647\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 0.585770\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.766285\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 0.376875\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.329258\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 0.416929\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.327820\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 0.501638\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.478754\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.406947\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.351134\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 0.280616\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.536124\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 0.473894\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.429002\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 0.232472\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.414877\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 0.500230\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.329024\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.288845\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.270664\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 0.355291\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.296120\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 0.292606\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.384910\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.422903\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.567657\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 0.388394\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.278091\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.262808\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.373303\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.242224\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.323353\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 0.214544\n",
      "\n",
      "Test set: Average loss: 0.0013, Accuracy: 9609/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.285250\n",
      "Train Epoch: 2 [1000/60000 (2%)]\tLoss: 0.236350\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.301273\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 0.398956\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.319627\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.396244\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.401854\n",
      "Train Epoch: 2 [7000/60000 (12%)]\tLoss: 0.358511\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.188670\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 0.240305\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.230713\n",
      "Train Epoch: 2 [11000/60000 (18%)]\tLoss: 0.310259\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.276828\n",
      "Train Epoch: 2 [13000/60000 (22%)]\tLoss: 0.292935\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.238239\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.404842\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.309674\n",
      "Train Epoch: 2 [17000/60000 (28%)]\tLoss: 0.215152\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.267858\n",
      "Train Epoch: 2 [19000/60000 (32%)]\tLoss: 0.271137\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.254036\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: 0.252797\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.274893\n",
      "Train Epoch: 2 [23000/60000 (38%)]\tLoss: 0.399723\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.303266\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.310930\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.330031\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: 0.470804\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.264401\n",
      "Train Epoch: 2 [29000/60000 (48%)]\tLoss: 0.227131\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.453000\n",
      "Train Epoch: 2 [31000/60000 (52%)]\tLoss: 0.255808\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.345478\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: 0.319573\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.384280\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.135490\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.272838\n",
      "Train Epoch: 2 [37000/60000 (62%)]\tLoss: 0.228588\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.299802\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: 0.308960\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.272703\n",
      "Train Epoch: 2 [41000/60000 (68%)]\tLoss: 0.300953\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.271292\n",
      "Train Epoch: 2 [43000/60000 (72%)]\tLoss: 0.310871\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.331744\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.236130\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.351482\n",
      "Train Epoch: 2 [47000/60000 (78%)]\tLoss: 0.164640\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.275395\n",
      "Train Epoch: 2 [49000/60000 (82%)]\tLoss: 0.220209\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.462272\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: 0.241818\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.229861\n",
      "Train Epoch: 2 [53000/60000 (88%)]\tLoss: 0.315458\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.186084\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.182516\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.447539\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: 0.099334\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.213538\n",
      "Train Epoch: 2 [59000/60000 (98%)]\tLoss: 0.261166\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m      2\u001b[0m     train(epoch)\n\u001b[1;32m----> 3\u001b[0m     test()\n",
      "Cell \u001b[1;32mIn[12], line 31\u001b[0m, in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():                                            \u001b[38;5;66;03m# disable gradient calculation to speed up the computation as we don't need gradients for evaluation\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     32\u001b[0m         data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1318\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[1;32m-> 1318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_workers()\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m \n\u001b[0;32m   1323\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[1;32m-> 1443\u001b[0m     w\u001b[38;5;241m.\u001b[39mjoin(timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL)\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[0;32m   1445\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\multiprocessing\\process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\Lib\\multiprocessing\\popen_spawn_win32.py:112\u001b[0m, in \u001b[0;36mPopen.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     msecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m--> 112\u001b[0m res \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForSingleObject(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle), msecs)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWAIT_OBJECT_0:\n\u001b[0;32m    114\u001b[0m     code \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mGetExitCodeProcess(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m data, target \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "data, target = test_data[1]\n",
    "\n",
    "data = data.unsqueeze(0).to(device)\n",
    "output = model(data)\n",
    "pred = output.argmax(dim = 1, keepdim = True).item()\n",
    "\n",
    "print(f'Prediction: {pred}')\n",
    "\n",
    "plt.imshow(data.cpu().numpy().squeeze(), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
